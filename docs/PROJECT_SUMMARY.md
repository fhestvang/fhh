# Project Summary: dlt-dbt-jaffle-shop

## ğŸ“¦ What Has Been Created

A complete, production-ready workspace for testing **dlt** (data load tool) and **dbt** (data build tool) with the **Jaffle Shop** demo dataset and **DuckDB**.

## ğŸ—ï¸ Project Structure

```
dlt-dbt-jaffle-shop/
â”œâ”€â”€ ğŸ“‹ Configuration & Setup
â”‚   â”œâ”€â”€ .env.example              # Environment variables template
â”‚   â”œâ”€â”€ pyproject.toml            # Python dependencies (uv)
â”‚   â”œâ”€â”€ Dockerfile                # Docker image configuration
â”‚   â”œâ”€â”€ docker-compose.yml        # Docker Compose setup
â”‚   â””â”€â”€ .devcontainer/            # VS Code Dev Container config
â”‚
â”œâ”€â”€ ğŸ”§ Claude Code Integration
â”‚   â””â”€â”€ .claude/
â”‚       â”œâ”€â”€ claude.md             # Project documentation for Claude
â”‚       â””â”€â”€ mcp.json              # Git MCP server configuration
â”‚
â”œâ”€â”€ ğŸ“Š Data Pipeline
â”‚   â”œâ”€â”€ dlt_pipelines/            # Data loading (Extract & Load)
â”‚   â”‚   â”œâ”€â”€ jaffle_shop_pipeline.py
â”‚   â”‚   â””â”€â”€ .dlt/config.toml
â”‚   â”‚
â”‚   â””â”€â”€ dbt_project/              # Data transformation (Transform)
â”‚       â”œâ”€â”€ dbt_project.yml
â”‚       â”œâ”€â”€ profiles.yml.example
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ staging/          # Staging layer (views)
â”‚           â”‚   â”œâ”€â”€ stg_customers.sql
â”‚           â”‚   â”œâ”€â”€ stg_orders.sql
â”‚           â”‚   â”œâ”€â”€ stg_payments.sql
â”‚           â”‚   â””â”€â”€ schema.yml
â”‚           â””â”€â”€ marts/            # Business logic (tables)
â”‚               â””â”€â”€ core/
â”‚                   â”œâ”€â”€ customers.sql
â”‚                   â”œâ”€â”€ orders.sql
â”‚                   â””â”€â”€ schema.yml
â”‚
â”œâ”€â”€ ğŸ› ï¸ Utilities
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ download_jaffle_data.py    # Download demo CSV files
â”‚   â”‚   â”œâ”€â”€ run_full_pipeline.py       # Run complete pipeline
â”‚   â”‚   â””â”€â”€ reset_database.py          # Reset database state
â”‚   â””â”€â”€ Makefile                       # Make commands for common tasks
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md                 # Main project documentation
â”‚   â”œâ”€â”€ QUICKSTART.md            # 5-minute quick start guide
â”‚   â”œâ”€â”€ CONTRIBUTING.md          # Contribution guidelines
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ ARCHITECTURE.md      # Technical architecture details
â”‚   â”‚   â””â”€â”€ SETUP.md             # Detailed setup instructions
â”‚   â””â”€â”€ LICENSE                  # MIT License
â”‚
â””â”€â”€ ğŸ’¾ Data
    â””â”€â”€ data/                    # DuckDB database & CSV files
        â””â”€â”€ .gitkeep
```

## ğŸ¯ Key Features

### 1. **Modern Data Stack**
- **dlt**: Python-based data loading framework
- **dbt**: SQL-based data transformation framework
- **DuckDB**: Fast, embedded analytical database
- **uv**: Lightning-fast Python package manager

### 2. **Multiple Development Options**
- âœ… Local development with uv
- âœ… Docker containerization
- âœ… VS Code Dev Containers
- âœ… Cross-platform support (Windows, macOS, Linux)

### 3. **Best Practices Implementation**
- Layered data architecture (raw â†’ staging â†’ marts)
- Data quality tests with dbt
- Environment variable management
- Version control with Git
- Comprehensive documentation
- Helper scripts for common tasks

### 4. **Claude Code Integration**
- Git MCP server configured
- Project-specific Claude documentation
- Optimized for AI-assisted development

### 5. **Complete Documentation**
- Quick start guide (5 minutes to running pipeline)
- Detailed setup instructions
- Architecture documentation
- Contributing guidelines
- Troubleshooting guides

## ğŸš€ Getting Started

### Quick Start (3 commands)
```bash
cd dlt-dbt-jaffle-shop
uv sync && cp .env.example .env && cp dbt_project/profiles.yml.example dbt_project/profiles.yml
uv run python scripts/download_jaffle_data.py && uv run python scripts/run_full_pipeline.py
```

### With Makefile (Linux/macOS/WSL)
```bash
cd dlt-dbt-jaffle-shop
make install setup download-data full-pipeline
```

## ğŸ“Š Data Pipeline Flow

```
CSV Files (Jaffle Shop)
    â†“
dlt Pipeline (Extract & Load)
    â†“
DuckDB (jaffle_shop_raw schema)
    â†“
dbt Staging Models (Clean & Standardize)
    â†“
dbt Marts Models (Business Logic)
    â†“
Analytics-Ready Tables
```

### Pipeline Components

**Input Data (CSV)**:
- `raw_customers.csv` - Customer information
- `raw_orders.csv` - Order transactions
- `raw_payments.csv` - Payment details

**dlt Pipeline**:
- Loads CSVs into DuckDB
- Creates `jaffle_shop_raw` schema
- Handles schema inference and typing

**dbt Staging Layer** (Views):
- `stg_customers` - Cleaned customer data
- `stg_orders` - Cleaned order data
- `stg_payments` - Cleaned payment data with amount conversion

**dbt Marts Layer** (Tables):
- `customers` - Customer 360 view with metrics
- `orders` - Order details with payment breakdowns

## ğŸ”§ Available Commands

### Using Helper Scripts
```bash
# Download demo data
uv run python scripts/download_jaffle_data.py

# Run complete pipeline
uv run python scripts/run_full_pipeline.py

# Reset database
uv run python scripts/reset_database.py
```

### Using Makefile
```bash
make install          # Install dependencies
make setup            # Set up environment files
make download-data    # Download demo data
make run-dlt         # Run dlt pipeline
make run-dbt         # Run dbt models
make test-dbt        # Run dbt tests
make full-pipeline   # Run complete pipeline
make reset           # Reset database
make docs            # View dbt documentation
```

### Manual Commands
```bash
# dlt pipeline
uv run python dlt_pipelines/jaffle_shop_pipeline.py

# dbt commands
cd dbt_project
uv run dbt run                    # Run all models
uv run dbt test                   # Run all tests
uv run dbt run --select staging.* # Run staging models
uv run dbt docs generate          # Generate docs
uv run dbt docs serve             # Serve docs
```

## ğŸ“ Key Files

| File | Purpose |
|------|---------|
| [QUICKSTART.md](QUICKSTART.md) | 5-minute quick start guide |
| [README.md](README.md) | Main project documentation |
| [docs/SETUP.md](docs/SETUP.md) | Detailed setup instructions |
| [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) | Architecture details |
| [pyproject.toml](pyproject.toml) | Python dependencies |
| [.claude/mcp.json](.claude/mcp.json) | MCP server configuration |
| [dlt_pipelines/jaffle_shop_pipeline.py](dlt_pipelines/jaffle_shop_pipeline.py) | dlt data loading |
| [dbt_project/models/](dbt_project/models/) | dbt transformation models |

## ğŸ“ Learning Resources

The project includes example implementations of:
- **dlt**: CSV ingestion, schema inference, DuckDB integration
- **dbt**: Staging models, marts, tests, documentation
- **DuckDB**: Embedded database for analytics
- **Docker**: Containerized development environment
- **uv**: Modern Python dependency management
- **MCP**: Model Context Protocol for Claude integration

## ğŸ” What You Can Do Next

1. **Explore the data**:
   ```python
   import duckdb
   conn = duckdb.connect('data/jaffle_shop.duckdb')
   print(conn.execute("SELECT * FROM marts.customers LIMIT 5").fetchdf())
   ```

2. **Modify dbt models**:
   - Edit SQL files in `dbt_project/models/`
   - Run `cd dbt_project && uv run dbt run`
   - View changes in database

3. **Add new data sources**:
   - Add resources to `dlt_pipelines/jaffle_shop_pipeline.py`
   - Create corresponding dbt staging models
   - Build new mart models

4. **Connect BI tools**:
   - Point Metabase, Tableau, or Superset to `data/jaffle_shop.duckdb`
   - Query `marts.*` tables

5. **Extend the pipeline**:
   - Add incremental loading
   - Implement more complex transformations
   - Add data quality checks

## ğŸ›¡ï¸ Best Practices Implemented

- âœ… **Separation of concerns**: Extract, Load, Transform separated
- âœ… **Layered architecture**: Raw â†’ Staging â†’ Marts
- âœ… **Data quality**: dbt tests for constraints
- âœ… **Documentation**: Inline docs, README, guides
- âœ… **Version control**: Git with proper .gitignore
- âœ… **Environment management**: .env files, profiles
- âœ… **Code quality**: Ruff for Python linting/formatting
- âœ… **Containerization**: Docker for reproducibility
- âœ… **Developer experience**: Multiple setup options, helper scripts

## ğŸ“ˆ Scalability Path

The project is designed to scale:

**Current**: Local development, demo data, single-file database
**Next**: PostgreSQL/MySQL, larger datasets, scheduled runs
**Future**: Cloud data warehouse (Snowflake/BigQuery), Airflow orchestration

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on:
- Code style
- Adding features
- Testing changes
- Submitting pull requests

## ğŸ“ Support

- Read [QUICKSTART.md](QUICKSTART.md) for quick setup
- Check [docs/SETUP.md](docs/SETUP.md) for detailed instructions
- Review [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for technical details
- Check troubleshooting sections in documentation

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file

---

**Project created with Claude Code**
**Ready for development, testing, and learning!**
